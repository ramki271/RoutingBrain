# ── Virtual Model Registry ────────────────────────────────────────────────────
# Policies reference virtual model IDs (rb://) only.
# When a new model version is released, only update the mapping here —
# never touch the routing policy YAML files.
#
# Virtual ID convention:
#   rb://<tier>_<specialty>
#
virtual_models:
  # ── Fast / Cheap tier ────────────────────────────────────────────────────
  rb://fast_cheap_code:
    provider: anthropic
    model: claude-haiku-4-5-20251001
    description: "Fast, cheap coding — boilerplate, simple generation"

  rb://fast_cheap_general:
    provider: anthropic
    model: claude-haiku-4-5-20251001
    description: "Fast, cheap general tasks"

  rb://fast_cheap_qa:
    provider: gemini
    model: gemini-2.0-flash
    description: "Lowest latency Q&A and lookups"

  rb://fast_cheap_review:
    provider: openai
    model: gpt-4o-mini
    description: "Fast pattern-matching code review"

  rb://fast_cheap_docs:
    provider: anthropic
    model: claude-haiku-4-5-20251001
    description: "Simple docstrings and inline comments"

  rb://fast_cheap_oss:
    provider: ollama
    model: codellama:34b
    description: "Self-hosted OSS code model — data never leaves infra"

  # ── Balanced tier ────────────────────────────────────────────────────────
  rb://balanced_code:
    provider: anthropic
    model: claude-sonnet-4-5-20250929
    description: "Best code quality/cost balance"

  rb://balanced_reasoning:
    provider: anthropic
    model: claude-sonnet-4-5-20250929
    description: "Nuanced reasoning for reviews, debugging, docs"

  rb://balanced_data:
    provider: gemini
    model: gemini-2.0-pro
    description: "Large context data analysis (2M token window)"

  rb://balanced_general:
    provider: openai
    model: gpt-4o
    description: "Balanced general purpose"

  # ── Powerful tier ────────────────────────────────────────────────────────
  rb://powerful_code:
    provider: anthropic
    model: claude-opus-4-5-20251101
    description: "Novel algorithms, complex architecture code"

  rb://powerful_reasoning:
    provider: anthropic
    model: claude-opus-4-5-20251101
    description: "Deep trade-off analysis, architecture decisions"

  rb://powerful_math:
    provider: openai
    model: o1
    description: "Extended mathematical and algorithmic reasoning"

  rb://powerful_debug:
    provider: openai
    model: o1
    description: "Extended thinking for hard bugs and race conditions"

  # ── Local OSS (self-hosted, data never leaves infra) ─────────────────────
  rb://local_general:
    provider: ollama
    model: llama3.1:70b
    description: "Self-hosted general tasks — safe for all data"

  rb://local_code:
    provider: ollama
    model: codellama:34b
    description: "Self-hosted code generation — safe for proprietary code"

  rb://local_code_specialized:
    provider: ollama
    model: deepseek-coder:33b
    description: "Self-hosted code specialist — competitive with GPT-4 on benchmarks"


# ── Concrete Model Registry ───────────────────────────────────────────────────
models:
  # Anthropic
  - model_id: claude-haiku-4-5-20251001
    provider: anthropic
    tier: fast_cheap
    input_cost_per_mtok: 0.80
    output_cost_per_mtok: 4.00
    context_window: 200000
    supports_streaming: true
    supports_tools: true

  - model_id: claude-sonnet-4-5-20250929
    provider: anthropic
    tier: balanced
    input_cost_per_mtok: 3.00
    output_cost_per_mtok: 15.00
    context_window: 200000
    supports_streaming: true
    supports_tools: true

  - model_id: claude-opus-4-5-20251101
    provider: anthropic
    tier: powerful
    input_cost_per_mtok: 15.00
    output_cost_per_mtok: 75.00
    context_window: 200000
    supports_streaming: true
    supports_tools: true

  # OpenAI
  - model_id: gpt-4o-mini
    provider: openai
    tier: fast_cheap
    input_cost_per_mtok: 0.15
    output_cost_per_mtok: 0.60
    context_window: 128000
    supports_streaming: true
    supports_tools: true

  - model_id: gpt-4o
    provider: openai
    tier: balanced
    input_cost_per_mtok: 2.50
    output_cost_per_mtok: 10.00
    context_window: 128000
    supports_streaming: true
    supports_tools: true

  - model_id: o1
    provider: openai
    tier: powerful
    input_cost_per_mtok: 15.00
    output_cost_per_mtok: 60.00
    context_window: 128000
    supports_streaming: false
    supports_tools: false

  # Google Gemini
  - model_id: gemini-2.0-flash
    provider: gemini
    tier: fast_cheap
    input_cost_per_mtok: 0.10
    output_cost_per_mtok: 0.40
    context_window: 1000000
    supports_streaming: true
    supports_tools: true

  - model_id: gemini-2.0-pro
    provider: gemini
    tier: balanced
    input_cost_per_mtok: 2.00
    output_cost_per_mtok: 8.00
    context_window: 2000000
    supports_streaming: true
    supports_tools: true

  # Local OSS (Ollama)
  - model_id: llama3.1:70b
    provider: ollama
    tier: local
    input_cost_per_mtok: 0.00
    output_cost_per_mtok: 0.00
    context_window: 128000
    supports_streaming: true
    supports_tools: false

  - model_id: codellama:34b
    provider: ollama
    tier: local
    input_cost_per_mtok: 0.00
    output_cost_per_mtok: 0.00
    context_window: 100000
    supports_streaming: true
    supports_tools: false

  - model_id: deepseek-coder:33b
    provider: ollama
    tier: local
    input_cost_per_mtok: 0.00
    output_cost_per_mtok: 0.00
    context_window: 100000
    supports_streaming: true
    supports_tools: false
